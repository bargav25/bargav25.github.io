---
title: "Context Parallel on Hybrid Models"
author: "Bargav Jagatha"
date: "2025-12-22"
format:
  html:
    toc: true
    code-fold: false
    css: blog-style.css
---

## Introduction

Context parallelism is a parallelization strategy for transformers that splits work along the sequence (token) dimension so multiple devices process different chunks of the same context, enabling longer context lengths and lower memory per device.

## Core Idea

The input sequence is partitioned into contiguous chunks (e.g., 8K tokens split into 4K+4K across two GPUs), and all intermediate activations are also sharded along this sequence dimension

Each GPU computes attention and feedforward for its own tokens, while exchanging necessary information so the result is mathematically equivalent to full attention over the entire sequence.

The most important concept to understand context parallel better is Ring Attention. Ring Attention shuffles the KV shards and calculates the partial attention scores, repeats until all KV shards have been used on each device.

But Ring Attention is designed for softmax scaled attention. Nowadays, with the increase in hybrid models (Mamba2, Gated DeltaNet, Kimi DeltaNet), we felt its important to have an opensource implementation of context parallel mechanism designed for those attentions.


## Scaled Dot-Product Attention

The attention mechanism can be described by the following equation:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Where:
- $Q$ is the query matrix
- $K$ is the key matrix  
- $V$ is the value matrix
- $d_k$ is the dimension of the key vectors

## Self-Attention

In self-attention, all three matrices come from the same source:

$$
Q = XW^Q, \quad K = XW^K, \quad V = XW^V
$$

## Multi-Head Attention

Multi-head attention allows the model to jointly attend to information from different representation subspaces:

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
$$

where each head is computed as:

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

## Example Code

Here's a simple implementation in PyTorch:

```python
import torch
import torch.nn as nn
import math

class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super().__init__()
        self.d_k = d_k
    
    def forward(self, Q, K, V, mask=None):
        # Compute attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # Apply mask if provided
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Apply softmax
        attention_weights = torch.softmax(scores, dim=-1)
        
        # Apply attention to values
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
```

## Complexity Analysis

The computational complexity of self-attention is $O(n^2 \cdot d)$, where:
- $n$ is the sequence length
- $d$ is the model dimension

This quadratic complexity becomes a bottleneck for long sequences, which is why sub-quadratic attention mechanisms like linear attention are being explored.

## Conclusion

Understanding the mathematical foundations of attention is crucial for working with modern transformer architectures. The scaling factor $\frac{1}{\sqrt{d_k}}$ prevents the softmax function from having extremely small gradients when $d_k$ is large.
